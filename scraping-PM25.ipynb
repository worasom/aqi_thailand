{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Pollution Data from Website\n",
    "\n",
    "Environmental agencies often developed countries make the pollution publicly available, however, this is not the case for many developing countries.  Thailand environmental protection agency(EPA), only one month history is available through their website without special request. ref[3]( http://www.aqmthai.com/public_report.php). Luckily, some historical data can be found in Berkeley Earth website. \n",
    "\n",
    "This notebok demonstrates how to automatically download Thailand's pollution data from Berkeley Earth's database ref [1](http://berkeleyearth.lbl.gov/air-quality/maps/cities/Thailand/) using requests, beautifulsoup and wget libraries. I also has another [notebook](https://github.com/worasom/aqi_thailand/blob/master/webscraping-AQI.ipynb) demonstrating how to scrap data from Thai EPA directly.\n",
    "\n",
    "\n",
    "Reference\n",
    "1. http://berkeleyearth.lbl.gov/air-quality/maps/cities/Thailand/\n",
    "2. https://automatetheboringstuff.com/chapter11/ \n",
    "3. http://www.aqmthai.com/public_report.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download PM2.5  data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berekeley Earth database looks like this. \n",
    "\n",
    "<img src=\"data/berekeley_earth1.png\" width=\"400\">\n",
    "\n",
    "The each folder contains airpollution data for each province in text and json format. There maybe more than one files in the folder, for example Nonthaburi province folder has three data files as shown here.\n",
    "\n",
    "<img src=\"data/berekeley_earth3.png\" width=\"500\">\n",
    "\n",
    "I am interested in obtaining data from Bangkok provinces and her neighbors. The website provides a list of Bangkok neighbors in Bangkok.neighbors.json file. Therefore, it more productive use Python to downloads these file automatically. In addition, I can reuse this work when I work Chiang Mai and its neighbors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import wget\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://berkeleyearth.lbl.gov/air-quality/maps/cities/Thailand/'\n",
    "res = requests.get(url)\n",
    "# create a soup object of Berkeley earth website \n",
    "soup = BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all provinces in this database\n",
    "provinces = soup.find_all(href=re.compile('/'))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the soup object of Bangkok link\n",
    "bkk = provinces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangkok/',\n",
       " 'Nonthaburi/',\n",
       " 'Samut_Prakan/',\n",
       " 'Samut_Sakhon/',\n",
       " 'Pathum_Thani/',\n",
       " 'Nakhon_Pathom/',\n",
       " 'Phra_Nakhon_Si_Ayutthaya/',\n",
       " 'Ratchaburi/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bulid a list of provinces to download the data \n",
    "to_grab = []\n",
    "to_grab.append(bkk['href'])\n",
    "# locate BKK neighbors json file\n",
    "nei_url = url+to_grab[0].replace('/','')+'.neighbors.json'\n",
    "bkk_j = requests.get(nei_url).json()\n",
    "nei = [prov[1]+'/' for prov in bkk_j if prov[0]=='Thailand']\n",
    "\n",
    "# some provinces in the list are not the province of interest's neighbors \n",
    "to_grab = to_grab+nei[:7]\n",
    "to_grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"Bangkok/\">Bangkok/</a>]\n",
      "[<a href=\"Nonthaburi/\">Nonthaburi/</a>]\n",
      "[<a href=\"Samut_Prakan/\">Samut_Prakan/</a>]\n",
      "[<a href=\"Samut_Sakhon/\">Samut_Sakhon/</a>]\n",
      "[<a href=\"Pathum_Thani/\">Pathum_Thani/</a>]\n",
      "[<a href=\"Nakhon_Pathom/\">Nakhon_Pathom/</a>]\n",
      "[<a href=\"Phra_Nakhon_Si_Ayutthaya/\">Phra_Nakhon_Si_Ayutthaya/</a>]\n",
      "[<a href=\"Ratchaburi/\">Ratchaburi/</a>]\n"
     ]
    }
   ],
   "source": [
    "for folder in to_grab: \n",
    "    # extract the href for data folders to download\n",
    "    link = soup.find_all(href = re.compile(folder))\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('data').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_txt(folder):\n",
    "    '''Input: soup object that contain the href for downloading data'''\n",
    "    grab_url = url+folder\n",
    "    prov_r = requests.get(grab_url)\n",
    "    prov_s = BeautifulSoup(prov_r.text)\n",
    "    for tag in prov_s.find_all(href=re.compile('.txt')):\n",
    "        data_url = grab_url+tag['href']\n",
    "        name = 'data/'+ tag['href']\n",
    "        wget.download(data_url,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 576938 / 576938"
     ]
    }
   ],
   "source": [
    "# start downloading !\n",
    "for folder in to_grab:\n",
    "    download_txt(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap Pollution Data for All Provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 613902 / 613902"
     ]
    }
   ],
   "source": [
    "for province in provinces:\n",
    "    download_txt(province['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
